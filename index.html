<!DOCTYPE html>
<html>

  <head>
    
    <meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta http-equiv="X-UA-Compatible" content="IE=edge">

<title>

  CATLAB


</title>
<meta name="description" content="Recent publications from CATLAB.
">

<!-- Open Graph -->


<!-- Bootstrap & MDB -->
<link href="https://stackpath.bootstrapcdn.com/bootstrap/4.5.2/css/bootstrap.min.css" rel="stylesheet" integrity="sha512-MoRNloxbStBcD8z3M/2BmnT+rg4IsMxPkXaGh2zD6LGNNFE80W3onsAhRcMAMrSoyWL9xD7Ert0men7vR8LUZg==" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/mdbootstrap/4.19.1/css/mdb.min.css" integrity="sha512-RO38pBRxYH3SoOprtPTD86JFOclM51/XTIdEPh5j8sj4tp8jmQIx26twG52UaLi//hQldfrh7e51WzP9wuP32Q==" crossorigin="anonymous" />

<!-- Fonts & Icons -->
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.14.0/css/all.min.css"  integrity="sha512-1PKOgIY59xJ8Co8+NE6FZ+LOAZKjy+KY8iq0G4B3CyeY6wYHN3yt9PW0XpSriVlkMXe40PTKnXrLnZ9+fkDaog==" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/academicons/1.9.0/css/academicons.min.css" integrity="sha512-W4yqoT1+8NLkinBLBZko+dFB2ZbHsYLDdr50VElllRcNt2Q4/GSs6u71UHKxB7S6JEMCp5Ve4xjh3eGQl/HRvg==" crossorigin="anonymous">
<link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons">
<link rel="preconnect" href="https://fonts.googleapis.com">
<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
<link href="https://fonts.googleapis.com/css2?family=Crimson+Text:wght@700&display=swap" rel="stylesheet">

<!-- Code Syntax Highlighting -->
<link rel="stylesheet" href="https://gitcdn.link/repo/jwarby/jekyll-pygments-themes/master/github.css" />

<!-- Styles -->

<link rel="icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>üêà‚Äç‚¨õ</text></svg>">

<link rel="stylesheet" href="/assets/css/main.css">
<link rel="canonical" href="/">

<!-- JQuery -->
<!-- jQuery -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js" integrity="sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==" crossorigin="anonymous"></script>


<!-- Theming-->






    
<!-- MathJax -->
<script type="text/javascript">
  window.MathJax = {
    tex: {
      tags: 'ams'
    }
  };
</script>
<script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script>
<script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>


  </head>

  <body class="fixed-top-nav ">

    <!-- Header -->

    <header>

    <!-- Nav Bar -->
   <!--  <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top">
    <div class="container">
      

      <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
        <span class="sr-only">Toggle navigation</span>
        <span class="icon-bar top-bar"></span>
        <span class="icon-bar middle-bar"></span>
        <span class="icon-bar bottom-bar"></span>
      </button>
       -->
      <div class="collapse navbar-collapse text-right" id="navbarNav">
        <ul class="navbar-nav ml-auto flex-nowrap">
          <!-- About -->
          <li class="nav-item active">
            <a class="nav-link" href="/">
              about
              
                <span class="sr-only">(current)</span>
              
            </a>
          </li>
          
          <!-- Blog -->
          <li class="nav-item ">
            <a class="nav-link" href="/blog/">
              blog
              
            </a>
          </li>
          
          <!-- Other pages -->
          
          
          
          
          
          
          
          
          
          
          
          
          <li class="nav-item ">
              <a class="nav-link" href="/publications/">
                CATLAB
                
              </a>
          </li>
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          <li class="nav-item ">
              <a class="nav-link" href="/projects/">
                projects
                
              </a>
          </li>
          
          
          
          
          
          <li class="nav-item dropdown ">
              <a class="nav-link dropdown-toggle" href="#" id="navbarDropdown" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">
                submenus
                
              </a>
              <div class="dropdown-menu dropdown-menu-right" aria-labelledby="navbarDropdown">
              
              
                <a class="dropdown-item" href="/publications/">publications</a>
              
              
              
                <div class="dropdown-divider"></div>
              
              
              
                <a class="dropdown-item" href="/projects/">projects</a>
              
              
              </div>
          </li>
          
          
          
          
          
          <li class="nav-item ">
              <a class="nav-link" href="/teaching/">
                teaching
                
              </a>
          </li>
          
          
          
          
        </ul>
      </div>
    
  

</header>


    <!-- Content -->

    <div class="container mt-5">
      <div class="post">

  <header class="post-header">

   <center><h1 class="post-title" style="font-family: 'Crimson Text', serif;">CATLAB</h1></center>

  <b>C</b>reative <b>A</b>I <b>T</b>echnologies research lab, currently hosted at <a href="https://cmpe.boun.edu.tr" target="_blank" rel="noopener noreferrer">Bogazici University, Department of Computer Engineering</a>. 
 
  </header>


  <article>


    
      <div class="publications">
     <h3 class="post-title" style="font-family: 'Crimson Text', serif;">Conference Publications</h3>
  <ol class="bibliography">
<li>
<div class="row" style="border: dotted 2px gray;">
  <div class="col-sm-4 abbr">
  
    <img src="assets/teaser/teaser_fantastic.jpeg" class="teaser img-fluid z-depth-1">
  
  
    
    <center>  <abbr class="badge">WACV</abbr>
</center>
    
  
  </div>

  <div id="fantasticstyles" class="col-sm-8">
    
      <div class="title">Fantastic Style Channels and Where to Find Them: A Submodular Framework for Discovering Diverse Directions in GANs</div>
      <div class="author">
        
          
          
          
          
          
          
            
              
                
                   Enis Simsar, 
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                   Umut Kocasari, 
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                   Ezgi Gulperi Er, 
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  and Pinar Yanardag 
                
              
            
          
        
      </div>

    <div class="periodical">
      
        <em>Winter Conference on Applications of Computer Vision</em>
      
      
        (<b>WACV</b>)
      
      
      , 2023
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a>
    
      
    
    
    
    
      
      <a href="https://arxiv.org/abs/2203.08516" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">PDF</a>
      
    
    
    
    
    
    
    
      <a href="/fantasticstyles" class="btn btn-sm z-depth-0" role="button">Project Page</a>
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>The discovery of interpretable directions in the latent spaces of pre-trained GAN models has recently become a popular topic. In particular, StyleGAN2 has enabled various image generation and manipulation tasks due to its rich and disentangled latent spaces. The discovery of such directions is typically done either in a supervised manner, which requires annotated data for each desired manipulation, or in an unsupervised manner, which requires a manual effort to identify the directions. As a result, existing work typically finds only a handful of directions in which controllable edits can be made. In this paper, we attempt to find the most representative and diverse subset of directions in stylespace of StyleGAN2. We formulate the problem as a coverage of stylespace and propose a novel submodular optimization framework that can be solved efficiently with a greedy optimization scheme. We evaluate our framework with qualitative and quantitative experiments and show that our method finds more diverse and relevant channels.</p>
    </div>
    

    <!-- Hidden bibtex block -->
    
  </div>
</div>
 </li>
<li>
<div class="row" style="border: dotted 2px gray;">
  <div class="col-sm-4 abbr">
  
    <img src="assets/teaser/teaser_latent3D.jpeg" class="teaser img-fluid z-depth-1">
  
  
    
    <center>  <abbr class="badge">WACV</abbr>
</center>
    
  
  </div>

  <div id="latent3D" class="col-sm-8">
    
      <div class="title">Text and Image Guided 3D Avatar Generation and Manipulation</div>
      <div class="author">
        
          
          
          
          
          
          
            
              
                
                   Zehranaz Canfes*, 
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                   Furkan Atasoy*, 
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                   Alara Dirik*, 
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  and Pinar Yanardag 
                
              
            
          
        
      </div>

    <div class="periodical">
      
        <em>Winter Conference on Applications of Computer Vision</em>
      
      
        (<b>WACV</b>)
      
      
      , 2023
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a>
    
      
    
    
    
    
      
      <a href="https://arxiv.org/abs/2202.06079" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">PDF</a>
      
    
    
    
    
    
    
    
      <a href="/latent3D" class="btn btn-sm z-depth-0" role="button">Project Page</a>
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>The manipulation of latent space has recently become an interesting topic in the field of generative models. Recent research shows that latent directions can be used to manipulate images towards certain attributes. However, controlling the generation process of 3D generative models remains a challenge. In this work, we propose a novel 3D manipulation method that can manipulate both the shape and texture of the model using text or image-based prompts such as ‚Äôa young face‚Äô or  ‚Äôa surprised face‚Äô. We leverage the power of  Contrastive Language-Image Pre-training (CLIP) model and a pre-trained 3D GAN model designed to generate face avatars, and create a fully differentiable rendering pipeline to manipulate meshes. More specifically, our method takes an input latent code and modifies it such that the target attribute specified by a text or image prompt is present or enhanced, while leaving other attributes largely unaffected. Our method requires only 5 minutes per manipulation, and we demonstrate the effectiveness of our approach with extensive results and comparisons. </p>
    </div>
    

    <!-- Hidden bibtex block -->
    
  </div>
</div>
 </li>
<li>
<div class="row" style="border: dotted 2px gray;">
  <div class="col-sm-4 abbr">
  
    <img src="assets/teaser/teaser_fairstyle.png" class="teaser img-fluid z-depth-1">
  
  
    
    <center>  <abbr class="badge">ECCV</abbr>
</center>
    
  
  </div>

  <div id="fairstyle" class="col-sm-8">
    
      <div class="title">FairStyle: Debiasing StyleGAN2 with Style Channel Manipulations</div>
      <div class="author">
        
          
          
          
          
          
          
            
              
                
                   Cemre Efe Karakas*, 
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                   Alara Dirik*, 
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                   Eylul Yalcinkaya, 
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  and Pinar Yanardag 
                
              
            
          
        
      </div>

    <div class="periodical">
      
        <em>European Conference on Computer Vision</em>
      
      
        (<b>ECCV</b>)
      
      
      , 2022
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a>
    
      
    
    
    
    
      
      <a href="https://arxiv.org/abs/2202.06240" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">PDF</a>
      
    
    
    
    
    
    
    
      <a href="/fairstyle" class="btn btn-sm z-depth-0" role="button">Project Page</a>
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>Recent advances in generative adversarial networks have shown that it is possible to generate high-resolution and hyperrealistic images. However, the images produced by GANs are only as fair and representative as the datasets on which they are trained. In this paper, we propose a method for directly modifying a pre-trained StyleGAN2 model that can be used to generate a balanced set of images with respect to one (e.g., eyeglasses) or more attributes (e.g.,  gender and eyeglasses). Our method takes advantage of the style space of the StyleGAN2 model to perform disentangled control of the target attributes to be debiased. Our method does not require training additional models and directly debiases the GAN model, paving the way for its use in various downstream applications. Our experiments show that our method successfully debiases the GAN model within a few minutes without compromising the quality of the generated images.  </p>
    </div>
    

    <!-- Hidden bibtex block -->
    
  </div>
</div>
 </li>
<li>
<div class="row" style="border: dotted 2px gray;">
  <div class="col-sm-4 abbr">
  
    <img src="assets/teaser/teaser_midispace.png" class="teaser img-fluid z-depth-1">
  
  
    
    <center>  <abbr class="badge">ACM CC</abbr>
</center>
    
  
  </div>

  <div id="midispace" class="col-sm-8">
    
      <div class="title">MIDISpace: Finding Linear Directions in Latent Space for Music Generation (Honorable Mention Award)</div>
      <div class="author">
        
          
          
          
          
          
          
            
              
                
                   Meliksah Turker, 
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                   Alara Dirik, 
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  and Pinar Yanardag 
                
              
            
          
        
      </div>

    <div class="periodical">
      
        <em>ACM Creativity &amp; Cognition</em>
      
      
        (<b>ACM CC</b>)
      
      
      , 2022
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a>
    
      
    
    
    
    
    
    
    
    
    
    
      <a href="/midispace" class="btn btn-sm z-depth-0" role="button">Project Page</a>
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>While recent works have shown that it is possible to find disentangled directions in the latent space of image generation networks, finding directions in the latent space of sequential models for music generation remains a largely unexplored topic. In this work, we propose a method for discovering linear directions in the latent space of a music generating Variational Auto-Encoder (VAE). We use PCA, a statistical method to transform the input data such that the variation along the new axes is maximized. We apply PCA on the latent space activations of our model and find largely disentangled directions that change the style and characteristics of the input music. Our experiments show that the found directions are often monotonic, global and encode fundamental musical characteristics such as colorfulness, speed and repetitiveness. Moreover, we propose a set of quantitative metrics to describe different musical styles and characteristics to evaluate our results. We show that the found directions decouple content and can be utilized for style transfer and conditional music generation tasks.</p>
    </div>
    

    <!-- Hidden bibtex block -->
    
  </div>
</div>
 </li>
<li>
<div class="row" style="border: dotted 2px gray;">
  <div class="col-sm-4 abbr">
  
    <img src="assets/teaser/teaser_cat_stylemc.png" class="teaser img-fluid z-depth-1">
  
  
    
    <center>  <abbr class="badge">WACV</abbr>
</center>
    
  
  </div>

  <div id="stylemc" class="col-sm-8">
    
      <div class="title">StyleMC: Multi-Channel Based Fast Text-Guided Image Generation and Manipulation</div>
      <div class="author">
        
          
          
          
          
          
          
            
              
                
                   Umut Kocasari, 
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                   Alara Dirik, 
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                   Mert Tiftikci, 
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  and Pinar Yanardag 
                
              
            
          
        
      </div>

    <div class="periodical">
      
        <em>Winter Conference on Applications of Computer Vision</em>
      
      
        (<b>WACV</b>)
      
      
      , 2022
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a>
    
      
    
    
    
    
      
      <a href="https://arxiv.org/abs/2112.08493" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">PDF</a>
      
    
    
    
    
      <a href="https://github.com/catlab-team/stylemc" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Code</a>
    
    
    
    
      <a href="/stylemc" class="btn btn-sm z-depth-0" role="button">Project Page</a>
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>Pre-trained GANs have shown great potential for interpretable directions in the latent space. The discovery of such directions is often done in a supervised or self-supervised manner and requires manual annotations which limits their application in practice. On the other hand, unsupervised approaches provide a way to discover interpretable directions without any supervision, but no fine-grained attribute can be discovered. Recent work such as StyleCLIP aims to overcome this limitation by leveraging the power of CLIP, a joint representational model for text and images, for text-driven image manipulation. While promising, these methods take several hours of pre-processing or training time, and require multiple text prompts. In this work, we propose a fast and efficient method for text-guided image generation and manipulation by leveraging the power of StyleGAN2 and CLIP. Our method uses a CLIP-based loss and an identity loss to manipulate images via user-supplied text prompts without changing any of the irrelevant attributes. Unlike previous work, our method requires only 12 seconds of optimization per text prompt and can be used with any pre-trained StyleGAN2 model. We demonstrate the effectiveness of our method with extensive results and comparisons to state-of-the-art approaches.</p>
    </div>
    

    <!-- Hidden bibtex block -->
    
  </div>
</div>
 </li>
<li>
<div class="row" style="border: dotted 2px gray;">
  <div class="col-sm-4 abbr">
  
    <img src="assets/teaser/teaser_cat_latentclr.png" class="teaser img-fluid z-depth-1">
  
  
    
    <center>  <abbr class="badge">ICCV</abbr>
</center>
    
  
  </div>

  <div id="latentclr" class="col-sm-8">
    
      <div class="title">LatentCLR: A Contrastive Learning Approach for Unsupervised Discovery of Interpretable Directions</div>
      <div class="author">
        
          
          
          
          
          
          
            
              
                
                   Oguz Kaan Yuksel*, 
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                   Enis Simsar*, 
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                   Ezgi Gulperi Er, 
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  and Pinar Yanardag 
                
              
            
          
        
      </div>

    <div class="periodical">
      
        <em>International Conference on Computer Vision</em>
      
      
        (<b>ICCV</b>)
      
      
      , 2021
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a>
    
      
    
    
    
    
      
      <a href="https://arxiv.org/abs/2104.00820" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">PDF</a>
      
    
    
    
    
      <a href="https://github.com/catlab-team/latentclr" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Code</a>
    
    
    
    
      <a href="/latentclr" class="btn btn-sm z-depth-0" role="button">Project Page</a>
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>Recent research has shown great potential for finding interpretable directions in the latent spaces of pre-trained Generative Adversarial Networks (GANs). These directions provide controllable generation and support a wide range of semantic editing operations such as zoom or rotation. The discovery of such directions is often performed in a supervised or semi-supervised fashion and requires manual annotations,  limiting their applications in practice. In comparison, unsupervised discovery enables finding subtle directions a priori hard to recognize. In this work, we propose a contrastive-learning-based approach for discovering semantic directions in the latent space of pretrained GANs in a self-supervised manner. Our approach finds semantically meaningful dimensions compatible with state-of-the-art methods.</p>
    </div>
    

    <!-- Hidden bibtex block -->
    
  </div>
</div>
 </li>
</ol>
</div>

    

   
      <div class="publications">
     <h3 class="post-title" style="font-family: 'Crimson Text', serif;">Workshop Publications</h3>
  <ol class="bibliography">
<li>
<div class="row" style="border: dotted 2px gray;">
  <div class="col-sm-4 abbr">
  
    <img src="assets/teaser/teaser_paintinstyle.png" class="teaser img-fluid z-depth-1">
  
  
    
    <center>  <abbr class="badge">CVPR Workshop</abbr>
</center>
    
  
  </div>

  <div id="paintinstyle" class="col-sm-8">
    
      <div class="title">PaintInStyle: One-Shot Discovery of Interpretable Directions by Painting</div>
      <div class="author">
        
          
          
          
          
          
          
            
              
                
                   Berkay Doner*, 
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                   Elif Sema Balcioglu*, 
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                   Merve Rabia Barin*, 
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                   Umut Kocasari, 
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                   Mert Tiftikci, 
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  and Pinar Yanardag 
                
              
            
          
        
      </div>

    <div class="periodical">
      
        <em>Conference on Computer Vision and Pattern Recognition</em>
      
      
        (<b>CVPR Workshop</b>)
      
      
      , 2022
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a>
    
      
    
    
    
    
    
    
    
    
    
    
      <a href="/paintinstyle" class="btn btn-sm z-depth-0" role="button">Project Page</a>
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>The search for interpretable directions in latent spaces of pre-trained Generative Adversarial Networks (GANs) has become a topic of interest. These directions can be utilized to perform semantic manipulations on the GAN generated images. The discovery of such directions is performed either in a supervised way, which requires manual annotation or pre-trained classifiers, or in an unsupervised way, which requires the user to interpret what these directions represent. Our goal in this work is to find meaningful latent space directions that can be used to manipulate images in a one-shot manner where the user provides a simple drawing (such as drawing a beard or painting a red lipstick) using basic image editing tools. Our method then finds a direction that can be applied to any latent vector to perform the desired edit. We demonstrate that our method is able to find several distinct and fine-grained directions in a variety of datasets.</p>
    </div>
    

    <!-- Hidden bibtex block -->
    
  </div>
</div>
 </li>
<li>
<div class="row" style="border: dotted 2px gray;">
  <div class="col-sm-4 abbr">
  
    <img src="assets/teaser/teaser_rankinstyle.jpeg" class="teaser img-fluid z-depth-1">
  
  
    
    <center>  <abbr class="badge">CVPR Workshop</abbr>
</center>
    
  
  </div>

  <div id="rankinstyle" class="col-sm-8">
    
      <div class="title">Rank in Style: A Ranking-based Approach to Find Interpretable Directions</div>
      <div class="author">
        
          
          
          
          
          
          
            
              
                
                   Umut Kocasari, 
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                   Kerem Zaman, 
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                   Mert Tiftikci, 
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                   Enis Simsar, 
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  and Pinar Yanardag 
                
              
            
          
        
      </div>

    <div class="periodical">
      
        <em>Conference on Computer Vision and Pattern Recognition</em>
      
      
        (<b>CVPR Workshop</b>)
      
      
      , 2022
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a>
    
      
    
    
    
    
    
    
    
    
    
    
      <a href="/paintinstyle" class="btn btn-sm z-depth-0" role="button">Project Page</a>
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>Recent work such as StyleCLIP aims to harness the power of CLIP embeddings for controlled manipulations. Although these models are capable of manipulating images based on a text prompt, the success of the manipulation often depends on careful selection of the appropriate text for the desired manipulation. This limitation makes it particularly difficult to perform text-based manipulations in domains where the user lacks expertise, such as fashion. To address this problem, we propose a method for automatically determining the most successful and relevant text-based edits using a pre-trained StyleGAN model. Our approach consists of a novel mechanism that uses CLIP to guide beam-search decoding, and a ranking method that identifies the most relevant and successful edits based on a list of keywords. We also demonstrate the capabilities of our framework in several domains, including fashion.</p>
    </div>
    

    <!-- Hidden bibtex block -->
    
  </div>
</div>
 </li>
<li>
<div class="row" style="border: dotted 2px gray;">
  <div class="col-sm-4 abbr">
  
    <img src="assets/teaser/teaser_theatergan.png" class="teaser img-fluid z-depth-1">
  
  
    
    <center>  <abbr class="badge">NeurIPS Workshop</abbr>
</center>
    
  
  </div>

  <div id="theatergan" class="col-sm-8">
    
      <div class="title">Controlled Cue Generation for Play Scripts (Best Paper Award)</div>
      <div class="author">
        
          
          
          
          
          
          
            
              
                
                   Alara Dirik*, 
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                   Hilal Donmez*, 
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  and Pinar Yanardag 
                
              
            
          
        
      </div>

    <div class="periodical">
      
        <em>Controllable Generative Modeling in Language and Vision</em>
      
      
        (<b>NeurIPS Workshop</b>)
      
      
      , 2021
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a>
    
      
    
    
    
    
      
      <a href="https://arxiv.org/abs/2112.06953" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">PDF</a>
      
    
    
    
    
    
    
    
      <a href="/cuegen" class="btn btn-sm z-depth-0" role="button">Project Page</a>
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>We propose the novel task of theatrical cue generation from dialogues. Using our play scripts dataset, which consists of 775K lines of dialogue and 277K cues, we approach the problem of cue generation as a controlled text generation task and show how cues can be used to amplify the impact of dialogue using a language model conditioned on a dialogue/cue discriminator. In addition, we explore the use of topic keywords and emotions to drive cue generation. Extensive quantitative and qualitative experiments show that language models can be successfully used to generate plausible and attribute-controlled text in highly specialized domains such as theater play scripts.</p>
    </div>
    

    <!-- Hidden bibtex block -->
    
  </div>
</div>
 </li>
<li>
<div class="row" style="border: dotted 2px gray;">
  <div class="col-sm-4 abbr">
  
    <img src="assets/teaser/teaser_creativegan.png" class="teaser img-fluid z-depth-1">
  
  
    
    <center>  <abbr class="badge">NeurIPS Workshop</abbr>
</center>
    
  
  </div>

  <div id="creativegan" class="col-sm-8">
    
      <div class="title">Exploring Latent Dimensions of Crowd-sourced Creativity</div>
      <div class="author">
        
          
          
          
          
          
          
            
              
                
                   Umut Kocasari*, 
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                   Alperen Bag*, 
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                   Efehan Atici, 
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  and Pinar Yanardag 
                
              
            
          
        
      </div>

    <div class="periodical">
      
        <em>Machine Learning for Creativity and Design</em>
      
      
        (<b>NeurIPS Workshop</b>)
      
      
      , 2021
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a>
    
      
    
    
    
    
      
      <a href="https://arxiv.org/abs/2112.06978" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">PDF</a>
      
    
    
    
    
      <a href="https://github.com/catlab-team/latentcreative" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Code</a>
    
    
    
    
      <a href="/latentcreative" class="btn btn-sm z-depth-0" role="button">Project Page</a>
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>Recent research showed that it is possible to find directions in the latent spaces of pre-trained GANs. These directions provide controllable generation and support a wide range of semantic editing operations such as zoom-in or rotation. While existing works focus on discovering directions for semantic image editing, we focus on an abstract property: Creativity. Can we manipulate an image to make it more or less creative? We build our work on the largest AI-based creativity platform Artbreeder where users are able to generate unique images using pre-trained GAN models. We explore the latent dimensions of the images generated on this platform and present a novel framework for manipulating images to make them more creative.</p>
    </div>
    

    <!-- Hidden bibtex block -->
    
  </div>
</div>
 </li>
<li>
<div class="row" style="border: dotted 2px gray;">
  <div class="col-sm-4 abbr">
  
    <img src="assets/teaser/teaser_graph2pix.png" class="teaser img-fluid z-depth-1">
  
  
    
    <center>  <abbr class="badge">ICCV Workshop</abbr>
</center>
    
  
  </div>

  <div id="graph2pix" class="col-sm-8">
    
      <div class="title">Graph2Pix: A Graph-Based Image to Image Translation Framework (Full Paper)</div>
      <div class="author">
        
          
          
          
          
          
          
            
              
                
                   Dilara Gokay*, 
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                   Enis Simsar*, 
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                   Efehan Atici, 
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                   Atif Emre Yilmaz, 
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                   Alper Ahmetoglu, 
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  and Pinar Yanardag 
                
              
            
          
        
      </div>

    <div class="periodical">
      
        <em>Advances of Image Manipulation</em>
      
      
        (<b>ICCV Workshop</b>)
      
      
      , 2021
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a>
    
      
    
    
    
    
      
      <a href="https://arxiv.org/abs/2108.09752" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">PDF</a>
      
    
    
    
    
      <a href="https://github.com/catlab-team/graph2pix" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Code</a>
    
    
    
    
      <a href="/graph2pix" class="btn btn-sm z-depth-0" role="button">Project Page</a>
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>In this paper, we propose a graph-based image-to-image translation framework for generating images. We use rich data collected from the popular creativity platform Artbreeder, where users interpolate multiple GAN-generated images to create artworks. This unique approach of creating new images leads to a tree-like structure where one can track historical data about the creation of a particular image. Inspired by this structure, we propose a novel graph-to-image translation model called Graph2Pix, which takes a graph and corresponding images as input and generates a single image as output. Our experiments show that Graph2Pix is able to outperform several image-to-image translation frameworks on benchmark metrics, including LPIPS (with a 25% improvement) and human perception studies (n=60), where users preferred the images generated by our method 81.5% of the time.</p>
    </div>
    

    <!-- Hidden bibtex block -->
    
  </div>
</div>
 </li>
</ol>
</div>

    

    
      <div class="news">
  <hr>
  <h2 class="post-title" style="font-family: 'Crimson Text', serif;">News</h2>
  
    <div class="table-responsive">
      <table class="table table-sm table-borderless">
      
      
        <tr>
          <th scope="row">Aug 8, 2022</th>
          <td>
            
              One paper is accepted to ECCV‚Äô22, two papers are accepted to WACV‚Äô23!

            
          </td>
        </tr>
      
        <tr>
          <th scope="row">Jun 28, 2022</th>
          <td>
            
              Midispace received Honorable Mention Award at ACM Creativity &amp; Cognition Conference! <img class="emoji" title=":trophy:" alt=":trophy:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f3c6.png" height="20" width="20">

            
          </td>
        </tr>
      
        <tr>
          <th scope="row">Jan 28, 2022</th>
          <td>
            
              CATLAB undergraduate researchers Cemre and Eylul  won the best senior thesis award! <img class="emoji" title=":sparkles:" alt=":sparkles:" src="https://github.githubassets.com/images/icons/emoji/unicode/2728.png" height="20" width="20">

            
          </td>
        </tr>
      
        <tr>
          <th scope="row">Dec 13, 2021</th>
          <td>
            
              <a href="https://catlab-team.github.io/cuegen" target="_blank">Controlled Cue Generation for Play Scripts</a> is selected as the <a href="https://ctrlgenworkshop.github.io/accepted_papers.html" target="_blank" rel="noopener noreferrer">Best Paper</a> at CTRLGen Workshop at NeurIPS‚Äô21! <img class="emoji" title=":trophy:" alt=":trophy:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f3c6.png" height="20" width="20">

            
          </td>
        </tr>
      
        <tr>
          <th scope="row">Aug 22, 2021</th>
          <td>
            
              One paper is accepted to ICCV, and one paper is accepted to WACV!

            
          </td>
        </tr>
      
      </table>
    </div>
  
</div>

    



<hr>
<h2 class="post-title" style="font-family: 'Crimson Text', serif;">Team</h2>

<b>Principal Investigator</b>:  <a href="http://pinguar.org" target="_blank" rel="noopener noreferrer">Dr. Pinar Yanardag</a><br>
<b>Grad students</b>: <a href="https://enis.dev" target="_blank" rel="noopener noreferrer">Enis Simsar</a> (MSc, part-time), Alara Dirik (MSc, part-time)<br>
<b>Undergrad students</b>: <a href="https://kocasariumut.github.io" target="_blank" rel="noopener noreferrer">Umut Kocasari</a>, Kerem Zaman, Zehranaz Canfes, Furkan Atasoy, Berkay Doner, Elif Sema Balcioglu, Merve Rabia Barin<br>
<b>Undergrad students <i>(alumni)</i>:</b> <a href="https://okyksl.gitlab.io/" target="_blank" rel="noopener noreferrer">Oguz Kaan Yuksel</a> (now at EPFL), Dilara Gokay (now at TUM), Mert Yuksekgonul (now at Stanford CS), Ezgi Gulperi Er (now at Goldman Sachs), Eylul Yalcinkaya (now at Facebook), Cemre Efe Karakas (now at Amazon)

<hr>
<h2 class="post-title" style="font-family: 'Crimson Text', serif;">Acknowledgements</h2>
We would like to acknowledge the support from 2232 International Fellowship for Outstanding Researchers Program of TUBITAK (Project No: 118c321). We also acknowledge the support of NVIDIA Corporation through the donation of the TITAN X GPU and GCP research credits from Google.  This website is using <a href="https://github.com/alshedivat/al-folio" target="_blank" rel="noopener noreferrer">al-folio</a> theme.

 


    
    <div class="social">
      <div class="contact-icons">
        <a href="mailto:%70%69%6E%61%72@%61%69-%66%69%63%74%69%6F%6E.%63%6F%6D"><i class="fas fa-envelope"></i></a>




<a href="https://github.com/catlab-team" title="GitHub" target="_blank" rel="noopener noreferrer"><i class="fab fa-github"></i></a>
<a href="https://instagram.com/howtogeneratealmostanything" title="Instagram" target="_blank" rel="noopener noreferrer"><i class="fab fa-instagram"></i></a>


<a href="https://youtube.com/channel/UCO5slRB0MXWNEgiOuXtFikg" title="Youtube" target="_blank" rel="noopener noreferrer"><i class="fab fa-youtube"></i></a>











      </div>
      <div class="contact-note">Drop us an e-mail or check us on social media.
</div>
    </div>
    


  </article>

</div>

    </div>

    <!-- Footer -->

    
<footer class="fixed-bottom">
  
</footer>



  </body>

  <!-- Bootsrap & MDB scripts -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/2.4.4/umd/popper.min.js" integrity="sha512-eUQ9hGdLjBjY3F41CScH3UX+4JDSI9zXeroz7hJ+RteoCaY+GP/LDoM8AO+Pt+DRFw3nXqsjh9Zsts8hnYv8/A==" crossorigin="anonymous"></script>
<script src="https://stackpath.bootstrapcdn.com/bootstrap/4.5.2/js/bootstrap.min.js" integrity="sha512-M5KW3ztuIICmVIhjSqXe01oV2bpe248gOxqmlcYrEzAvws7Pw3z6BK0iGbrwvdrUQUhi3eXgtxp5I8PDo9YfjQ==" crossorigin="anonymous"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/mdbootstrap/4.19.1/js/mdb.min.js" integrity="sha512-Mug9KHKmroQFMLm93zGrjhibM2z2Obg9l6qFG2qKjXEXkMp/VDkI4uju9m4QKPjWSwQ6O2qzZEnJDEeCw0Blcw==" crossorigin="anonymous"></script>

  
<!-- Mansory & imagesLoaded -->
<script defer src="https://unpkg.com/masonry-layout@4/dist/masonry.pkgd.min.js"></script>
<script defer src="https://unpkg.com/imagesloaded@4/imagesloaded.pkgd.min.js"></script>
<script defer src="/assets/js/mansory.js" type="text/javascript"></script>


  


<!-- Medium Zoom JS -->
<script src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.6/dist/medium-zoom.min.js" integrity="sha256-EdPgYcPk/IIrw7FYeuJQexva49pVRZNmt3LculEr7zM=" crossorigin="anonymous"></script>
<script src="/assets/js/zoom.js"></script>


<!-- Load Common JS -->
<script src="/assets/js/common.js"></script>


</html>
